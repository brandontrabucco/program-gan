## Abstract

Automated composition of computer programs has been a standing challenge since the early days of artificial intelligence, and has no global solutions with modern-day research in deep learning. Indeed, modeling context in language proves to be a difficult task singularly, and combining procedural knowledge in a generative fashion becomes an intractable problem in any turing complete programming language. There is a clear need for research in devising efficiently learned combinations of these independent problems. We investigate the nature of such an architecture, and determine the assumptions made by previous attempts at neural program synthesis and the limitations thereof. From our observations, we investigate an architecture using Variational Generative Adversarial Networks that builds separate learned models for language syntax, and semantic meaning. We employ this architecture to synthesize syntactically correct and semantically useful python source code from erroneous existing code.

## 1 - Introduction.

### 1.1 - Motivating our Research. 

There exist multiple reasons outside of the scientific domain that motivate research in neural program synthesis. The most obvious motivator is time efficiency for developing large computer programs that employ generic APIs such as for file conversion, handling http requests, and data analytics. Time saved by developers stimulates a proportional reduction of expense. Within a single firm, savings figures could measure in the hundreds of thousands over a single business year. A second obvious motivator is a reduction in the knowledge barrier for scientific computing, which can be argued to increase innovation trends and education standards, which induces higher productivity in research and industry. From a human supervisory perspective, we believe that neural program synthesis as opposed to induction is better suited to interface with industry professionals, and is a matter of dataset choice. Future research in this subject will certainly incur a deeper understanding of natural language.

### 1.2 - Deductive Programming.

Classical software engineering and algorithms design is predominantly deductive in view, where problems are first described abstractly, and specific programs exists as as a particular implementation of the general case (Kitzelmann E.). Starting with a generalization of the problem, and manipulating deductive rules to reach a specific solution constitutes proof that the result matches the original specification (Polozov O. et al.). Current state-of-the-art methods in deductive programming leverage a corpus of general source code, on which program features can be efficiently searched and imported. However, this approach lacks generalization for programs not represented in the program database or grammar tree (Balog, M. et al.). Abstraction learned by software engineers per se makes use of accumulated world knowledge, and is restrictively suited for information-rich environments. Associative generality is difficult to model with deep learning in practice, being that training examples are limited—consider Flash-Fill for Microsoft Excel, where a user provides the set of input-output examples (Devlin, J. et al.). Requiring less examples to synthesize accurate general programs is marketable, and necessary in future integration with deep learning. We conclude that classical program deduction approaches are not suited for our research.

### 1.3 - Inductive Programming. 

Recently successful methods for program synthesis are based on inductive reasoning, where specific cases point to a general problem description. These methods start with specific data such as input-output examples, test cases, and unstructured text, from which the best generalization of the data in program space is recovered. Induced programs identify patterns in the specification that may deviate the intended result; therefore, inductive techniques cannot be proven correct regarding their premises (Kitzelmann, E.). This method is perhaps better suited for deep learning, given that programs can be induced directly from a specification without requiring a procedural manipulation of logical axioms. State-of-the-art inductive programming methods operate under extreme data sparsity—fewer than ten input-output examples—and achieve high generalization performance on domain-specific language benchmarks (Devlin, J. et al.). Algorithm design is an active area of research in inductive programming, and deep learning is predominant, where program synthesis is framed as sequence translation from some descriptive memory structure (Reed, S. et al.). With these recent successes, we conclude that inductive programming is the proper domain for our research.

### 1.4 - Program Induction and Synthesis. 

Inductive programming has two primary branches of research that model either the execution of source code, namely Inductive Program Induction (IPI), or the composition of source code, namely Inductive Program Synthesis (IPS). These approaches vary significantly in choice of architecture, being that IPI learns to execute a procedure given from input-output examples, while IPS learns to write source code directly from some specification (Kitzelmann, E.). IPI methods have been proven difficult to optimize with plain gradient descent, and without an explicit representation of source code we conclude that these methods fundamentally lack human interpretability (Bunel, R. et al.). On the other hand, IPS methods suffer from similar optimization challenges. Recent strategies for IPS involve building fully differentiable programming languages, which can be optimized with respect to a specification requiring incremental gradient descent updates (Gaunt, A. et al.). Similar methods employ the same optimization technique with end-to-end differentiable interpreters for existing programming languages (Bosnjak, M. et al.). The process of traversing program space iteratively is time-expensive, and heavily dependent on hyperparameter choice to escape from locally optimal program solutions. We conclude that efficient generative IPS is an open challenge in deep learning, and is a suitable topic for our research.

## 2 - Neural Program Induction.

### 2.1 - Executing Programs. 

We have presented IPI and IPS as two branches of Inductive Programming, and we highlight that these methods address two fundamentally different tasks. Neural Program Induction (NPI) is an IPI technique for inducing latent procedures using Recurrent Neural Networks (RNNs). This technique is well suited for task-planning problems, where a program may constitute the policy of some robotic agent (Danfei, X. et al.). It is known that RNNs are turing complete such that, given sufficient training data and trainable parameters, they can approximate a procedure of arbitrary length and complexity. However, similar to the universal approximation theorem for artificial neural networks, RNNs prove difficult to optimize in practice, and suffer from information degradation across large temporal offsets (LeCun, Y. et al.). For this reason, a standard practice is to extend the RNN with an external memory structure, controlled by some trainable gating mechanism. The common implementation of this mechanism is with Long Short-Term Memory (Hochreiter S. et al.). This approach performs well for sequence learning tasks fewer than multiple dozen in length. However, sequences the order of hundreds suffer from the original information degradation problem. Sequence lengths in NPI tasks commonly exceed this margin, and various modifications to the RNN memory structure  have been proposed.

### 2.2 - External Memory. 

Memory structures proposed for NPI follow a different paradigm from the original RNN. Instead of local memory vectors for each layer of the RNN, recent approaches turn to global memory structures that memory enforce sparsity and resolution, enabling longer programs to be learned. The Neural Turing Machine (NTM) uses a neural network controller to read and write from an external memory matrix using a soft attention mechanism (Graves, A. et al.). This architecture learns simple array copying and sorting generalizing across sequence length. Similar to NTM in motivation, the Stack-RNN augments a neural network controller with stack-structured memory, and is able to learn simple arithmetic algorithms (Joulin, A. et al.). More recent work uses differentiable random-access memory for manipulating pointers and performing simple arithmetic operations (Kurach, K. et al.). These methods collectively suffer from a sequential computation overhead that is addressed by the shallow and parallel design of the Neural GPU (Kaiser, L. et al.). Likely due to their model complexity, these approaches require advanced regularization, including dropout, gradient noise, and curriculum or transfer learning in order to reasonably converge.

### 2.3 - Applications and Limitations. 

Architectures with global external memory have been trained to reproduce algorithms of mild complexity with varying degrees of success. Experiments with supervised and Q-learning show that these architectures are highly sensitive to choice in memory size, regularization factors, and controller network depth depending on the problem. Furthermore, determining these hyperparameters before training appears to be an open research topic (Zaremba, W. et al.). More constrained neural architectures such as the Neural Programmer-Interpreter have been proposed for sharing cross-task procedure embeddings, and achieve better cross-task generalization performance (Reed, S. et al.). However, these approaches remain difficult to optimize for recursive problems, and those of extreme sequence length (Cai, J. et al.). There does not exist any single architecture for NPI that is globally optimal for learning arbitrary procedures—such may be consider AI-complete. Instead, we discover a trade-off between cross-task generalization, training convergence, and information capacity, such that we conclude NPI remains an unsolved research topic.

## 3 - Neural Program Synthesis.

### 3.1 - Composing Programs. 

Framing Inductive Programming as learning to synthesize human-interpretable source code—instead of learning to reproduce the procedure represented by source code—fundamentally changes the problem. Neural Program Synthesis (NPS) is an IPS method for generating executable source code by traversing the space of possible program solutions for a given specification. Sufficiently optimized, guided search techniques such as DeepCoder are able to solve simple programming competition problems (Balog, M. et al.). Similarly, Deep API Programmer incrementally expands a program tree according to a guided search across the grammar for the Domain-Specific Language (DSL) and outperforms existing benchmarks for learning string transformations in Microsoft Excel (Bhupatiraju, S. et al.). An alternative approach tested on the same benchmark achieves similar performance by generating sequences of API tokens within the DSL, namely RobustFill (Devlin, J. et al). DeepAPI adapts the previous method by generating API tokens from the Java Development Kit, and is one of the first studies for NPI in modern high-level programming languages (Gu, X. et al.). These approaches have become  the industry standard for NPS, and are deployed in several Microsoft products (Polozov, O. et al.). However, these previous methods assume fixed API tokens, and fail to represent user-defined methods, and advanced control flow necessary to write basic programs in turing-complete  languages.

### 3.2 - Differentiable Program Graphs. 

More complete solutions for NPS require expressing more complex programs than compositions of fixed API tokens, and thus we naturally investigate methods for efficiently traversing the program space of programming languages with respect to some initial specification. Constructing a Differentiable Program Graph (DPG) that can be optimized directly using gradient descent has been the subject of recent research. Adaptive Neural Compilation descends the DPG with respect to an efficiency metric, producing algorithms specifically tuned for a given data distribution (Bunel, R. et al.). Similarly, a differentiable programming language TerpreT is able to synthesize programs with respect to input-output examples by the same method (Gaunt, A. et al.). These methods directly optimize the DPG using gradient descent on the program specification. However, when a partial program sketch is available, optimizing the partial DPG through some some differentiable interpreter proves to be more efficient (Bosnjak, M. et al.). Despite their advanced prospects, these previous techniques for manipulating the DPG are computationally expensive to implement and not commonly used in practice for NPS as a result.

### 3.3 - Applications and Limitations. 

For NPI, we recognize a natural progression of model complexity that incurs a proportional increase in induced program complexity, and cross-task generalization performance, but a similar trend is not observed within the NPS community. Instead, we find that guided search techniques and other IPS methods from outside deep learning achieve significantly higher performance compared with methods based on gradient descent (Gaunt, A. et al.). Furthermore, the sample inefficiency and optimization difficulty DPG manipulation is another key performance barrier. Other approaches such as guided search also suffer from low performance for complex compositional programs (Bhupatiraju, S. et al.). From this, we conclude that NPS research is semi-ununified in scope, that significant performance trade-offs hinder the relevance of more complex NPS architectures, and that designing a sample-efficient generative NPS architecture remains an attractive research challenge.

## 4 - The Generative Perspective.

### 4.1 - Problem Setting. 

In our investigation, we focus on building a generative language model for automatic synthesis of mathematical python functions. Python is an appropriate choice for program synthesis as a turing complete language, where previous work has focused with incomplete domain-specific languages such as string transformations as with Flash-Fill (Devlin, J. et al.). Our model operates with encoded vector descriptions of the desired program behavior. From these representations, our model composes executable python source code that matches our initial description. Having identified the learning target for our investigation, we then determine what knowledge is necessary to develop computer programs: an understanding of program syntax, and a prediction of program behavior. These characteristics constitute a new form of differentiable interpreter, different from prior methods that manipulate differentiable program graphs (Bosnjak, M. et al.). In our investigation, we explicitly learn separate models for these properties. However, previous methods in NPI discard program syntax, where other methods in NPS represent program behavior implicitly. We hypothesize that explicit models for these properties within programming languages enable generative NPS of complex programs.

### 4.2 - Syntax and Behavior. 

Describing the syntax rules that govern the placement of symbols in a certain language, and the descriptive behavior—commonly labeled as semantics—of those symbols is a historic research topic in the natural language community. Various encoding techniques are able to recover the semantic meaning of language symbols based on their relative placement (Mikolov, T. et al.). There exists a close relationship between language syntax rules and symbol behavior, therefore. By performing algebraic manipulations to vector space models of symbol behavior, we are able to efficiently traverse the syntax space of possible combinations of symbols (Turney, P. et al.). Framing program synthesis as natural language generation, we predict that a similar topology exists between program syntax and behavior, such that by constructing a differentiable model of syntax, and behavior, we can efficiently learn the set of possible programs that satisfy a certain initial specification. Therefore, explicit syntax and behavior models are necessary for generative  NPS.

*<p align="center">Idea by Brandon Trabucco on 2017.09.27, with collaboration from Ani Nrusimha and Ho Yin Chau</p>*
